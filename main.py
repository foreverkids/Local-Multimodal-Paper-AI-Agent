import os
import argparse
import shutil
import json
import time
from pypdf import PdfReader
from PIL import Image
import chromadb
from google import genai
from google.genai import types

# =================é…ç½®åŒºåŸŸ=================
GOOGLE_API_KEY = "AIzaSyCHd7JbWSb6q0291vfOwwIHlsjXmzxbl7M"

# ä¿®æ­£äº†æ‹¼å†™é”™è¯¯å¹¶å¢åŠ é»˜è®¤åˆ†ç±»
DEFAULT_TOPICS = "Computer Vision, NLP, Image Deblurring, Operating Systems, Continual Learning, Recommendation Systems"

# åˆå§‹åŒ–æ–°ç‰ˆ Client
client = genai.Client(api_key=GOOGLE_API_KEY)

# å‘é‡æ•°æ®åº“è·¯å¾„
DB_PATH = "./db"

# =================æ ¸å¿ƒåŠŸèƒ½ç±»=================

class LocalAIAgent:
    def __init__(self):
        # åˆå§‹åŒ– ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=DB_PATH)
        self.collection_papers = self.chroma_client.get_or_create_collection(name="papers")
        self.collection_images = self.chroma_client.get_or_create_collection(name="images")

    def _get_embedding(self, text, is_query=False):
        """ä½¿ç”¨æ–°ç‰ˆ SDK è·å–å‘é‡"""
        task_type = "RETRIEVAL_QUERY" if is_query else "RETRIEVAL_DOCUMENT"
        try:
            # æ–° SDK åµŒå…¥è°ƒç”¨æ–¹å¼
            result = client.models.embed_content(
                model="text-embedding-004",
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            return result.embeddings[0].values
        except Exception as e:
            print(f"âŒ Embedding Error: {e}")
            return None

    def add_paper(self, file_path, custom_topics=None, retry_count=0):
        if not os.path.exists(file_path): return

        MAX_RETRIES = 2
        print(f"ğŸ“„ Processing: {os.path.basename(file_path)}...")
        
        # 1. æå–æ–‡æœ¬
        text_content = ""
        try:
            reader = PdfReader(file_path)
            for page in reader.pages[:3]: 
                text = page.extract_text()
                if text: text_content += text + "\n"
        except Exception as e:
            print(f"âŒ PDF Read Error: {e}")
            return

        category = "Uncategorized"
        if len(text_content.strip()) > 50:
            topics = custom_topics if custom_topics else DEFAULT_TOPICS
            prompt = f"Classify this paper into ONE category: [{topics}, Others]. JSON: {{\"category\": \"Name\"}}. Text: {text_content[:5000]}"
            
            # --- æ ¸å¿ƒä¿®å¤é€»è¾‘ï¼šå°è¯•å¤šç§æ¨¡å‹åç§° ---
            # 1.5-flash åœ¨ä¸åŒç‰ˆæœ¬ SDK ä¸­å¯èƒ½æœ‰ä¸åŒçš„åˆ«å
            model_candidates = ["gemini-2.5-flash", "gemini-1.5-flash-latest", "gemini-1.5-flash-002"]
            
            success = False
            for model_name in model_candidates:
                try:
                    response = client.models.generate_content(
                        model=model_name, 
                        contents=prompt,
                        config=types.GenerateContentConfig(response_mime_type="application/json")
                    )
                    res_data = json.loads(response.text)
                    category = res_data.get("category", "Others")
                    print(f"ğŸ¤– AI Category: {category} (Model: {model_name})")
                    success = True
                    break # æˆåŠŸäº†å°±è·³å‡ºå¾ªç¯
                except Exception as e:
                    if "404" in str(e):
                        continue # 404 åˆ™å°è¯•ä¸‹ä¸€ä¸ªåç§°
                    elif "429" in str(e) and retry_count < MAX_RETRIES:
                        print(f"â³ Quota reached. Sleeping 10s...")
                        time.sleep(10)
                        return self.add_paper(file_path, custom_topics, retry_count + 1)
                    else:
                        print(f"âš ï¸ Model {model_name} failed: {e}")
            
            if not success:
                print("âŒ All model candidates failed. Attempting to list available models for you...")
                try:
                    # è¯Šæ–­ï¼šæ‰“å°å‡ºä½ å½“å‰ Key çœŸæ­£æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹
                    for m in client.models.list():
                        if "generateContent" in m.supported_generation_methods:
                            print(f"   ğŸ’¡ Available model: {m.name}")
                except: pass
                category = "Others"

        # 2. æ•´ç†æ–‡ä»¶ (è·¯å¾„ä¿æŒå’Œä½ ä¹‹å‰ä¸€è‡´)
        base_dir = "./paper"
        target_dir = os.path.join(base_dir, category)
        os.makedirs(target_dir, exist_ok=True)
        new_path = os.path.join(target_dir, os.path.basename(file_path))
        
        try:
            if os.path.abspath(file_path) != os.path.abspath(new_path):
                shutil.move(file_path, new_path)
                print(f"ğŸ“‚ Moved to: {new_path}")
        except: new_path = file_path

        # 3. å‘é‡å…¥åº“
        try:
            emb = self._get_embedding(text_content[:3000])
            if emb:
                self.collection_papers.add(
                    documents=[text_content[:3000]],
                    embeddings=[emb],
                    metadatas=[{"source": new_path, "category": category}],
                    ids=[new_path]
                )
                print("âœ… Indexed.")
        except Exception as e:
            print(f"âš ï¸ Embedding Error (Possibly Quota): {e}")
    def scan_dir(self, dir_path):
        """
        æ”¹è¿›çš„æ‰¹é‡æ‰«æï¼Œå¢åŠ å¼ºåˆ¶å»¶è¿Ÿ
        """
        print(f"ğŸš€ Scanning {dir_path}...")
        pdf_files = []
        for root, _, files in os.walk(dir_path):
            for f in files:
                if f.lower().endswith(".pdf"):
                    pdf_files.append(os.path.join(root, f))
        
        for i, pdf_path in enumerate(pdf_files):
            self.add_paper(pdf_path)
            # åœ¨å¤„ç†å®Œæ¯ä¸€ä¸ªæ–‡ä»¶åï¼Œå¼ºåˆ¶å¼ºåˆ¶å¼ºåˆ¶ä¼‘æ¯ 10 ç§’
            # å…è´¹ API å¿…é¡»ä½›ç³»å¤„ç†
            if i < len(pdf_files) - 1:
                print(f"â³ Cooling down for 7s (File {i+1}/{len(pdf_files)})...")
                time.sleep(7)

    def search_paper(self, query):
        emb = self._get_embedding(query, is_query=True)
        if not emb: return
        results = self.collection_papers.query(query_embeddings=[emb], n_results=3)
        print("\nğŸ” Search Results:")
        for i, meta in enumerate(results['metadatas'][0]):
            print(f"{i+1}. [{meta['category']}] {os.path.basename(meta['source'])}")

    def add_image(self, img_path):
        print(f"ğŸ–¼ï¸ Analyzing image: {img_path}...")
        try:
            img = Image.open(img_path)
            # æ–°ç‰ˆ SDK å¤šæ¨¡æ€è°ƒç”¨
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents=["Describe this image for semantic search.", img]
            )
            desc = response.text
            emb = self._get_embedding(desc)
            if emb:
                self.collection_images.add(
                    documents=[desc],
                    embeddings=[emb],
                    metadatas=[{"source": img_path}],
                    ids=[img_path]
                )
                print(f"âœ… Image Indexed: {desc[:50]}...")
        except Exception as e:
            print(f"âŒ Image Error: {e}")

    def search_image(self, query):
        emb = self._get_embedding(query, is_query=True)
        if not emb: return
        results = self.collection_images.query(query_embeddings=[emb], n_results=3)
        print("\nğŸ–¼ï¸ Image Results:")
        for i, meta in enumerate(results['metadatas'][0]):
            print(f"{i+1}. {meta['source']}")

# =================å…¥å£=================

def main():
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest="command")

    # è®ºæ–‡ç›¸å…³
    p_add = subparsers.add_parser("add_paper")
    p_add.add_argument("path")
    p_add.add_argument("--topics", default=None)

    p_scan = subparsers.add_parser("scan_dir")
    p_scan.add_argument("path")

    p_search = subparsers.add_parser("search_paper")
    p_search.add_argument("query")

    # å›¾åƒç›¸å…³
    i_add = subparsers.add_parser("add_image")
    i_add.add_argument("path")

    i_search = subparsers.add_parser("search_image")
    i_search.add_argument("query")

    args = parser.parse_args()
    agent = LocalAIAgent()

    if args.command == "add_paper": agent.add_paper(args.path, args.topics)
    elif args.command == "scan_dir": agent.scan_dir(args.path)
    elif args.command == "search_paper": agent.search_paper(args.query)
    elif args.command == "add_image": agent.add_image(args.path)
    elif args.command == "search_image": agent.search_image(args.query)
    else: parser.print_help()

if __name__ == "__main__":
    main()